# Mbpp Script for LLM analysis

## O que foi feito?
Basicamente, utilizei o dataset do [mbpp](https://huggingface.co/datasets/google-research-datasets/mbpp), que cont√©m descri√ß√µes de problemas de programa√ß√£o juntamente com solu√ß√£o e testes da solu√ß√£o, para observar como o Llama 3.1 70B cria sua pr√≥pria su√≠te de testes. O que fiz foi criar uma c√≥pia de cada problema removendo a su√≠te de testes e solicitando para a LLM criar a sua pr√≥pria. Uma vez isso feito, comparei o qu√£o bem sucedida a LLM se saiu principalmente em compara√ß√£o com os testes do pr√≥prio dataset que apontam sucesso nos seus c√≥digos de resolu√ß√£o de problemas.

## Como rodar?
1. Instalar a depend√™ncia do `Groq` com `pip`:
```bash
pip install groq
```
2. Crie sua pr√≥pria chave da API do Groq para poder rodar o Llama3.1 [aqui](https://console.groq.com/keys). Salve a chave em uma vari√°vel de ambiente com o nome `GROQ_API_KEY`
3. Execute o script no root do projeto o comando: 
```bash
python ./mbpp_script.py
```
> Os resultados estar√£o presentes na pasta `results`

## Interpreta√ß√µes de "erros na gera√ß√£o de teste"
- Em momentos de teste e an√°lise do conte√∫do "errado" da LLM, encontrei p√©rolas como `ERROR IN LLM TEST EXECUTION: assert is_not_prime(1) == True (question id: 3)`ü§¶‚Äç‚ôÇÔ∏è
- De quem √© a culpa? Bem, no fim das contas, devemos analisar tudo como "pode haver problema com":
  - LLM
  - C√≥digo da solu√ß√£o
  - Descri√ß√µes de problemas (potenciais "US's")
  - Prompt Engineer
  - Eu mesmo que posso ter feito o script com algum defeito rs (ningu√©m √© perfeito)
- Gostaria de ter removido todos os outros poss√≠veis causadores de problema da jogada para analisar apenas o desempenho da LLM, mas √© um dataset imenso para an√°lise manual...
- Acredito que uma boa pr√°tica √© analisar sempre os logs e o contexto antes de apontar o dedo para um desses cinco
- O arquivo `results/error_logs.txt` guarda todos os erros logados do script

## Melhorias em prompt
- o arquivo `results/prompt_history.md` guarda todo o hist√≥rico de prompts que utilizei quando o script estava finalizado, juntamente com alguns dados estat√≠sticos referentes a aquela execu√ß√£o em espec√≠fico. De modo geral, houve uma melhora significativa com adi√ß√£o de elementos de engenharia de prompt, como few-shot prompting, esclarecimento de output e afins. Quest√µes mais complexas foram mais dif√≠ceis de fazer a LLM passar, naturalmente.

## Resultados finais
Foram gerados aproximadamente 450 casos de teste para 90 problemas. De modo geral, os resultados m√©dios obtidos com 10 runs desse script foram:
- Testes compilados com sucesso: 90%
- Testes que aprovaram o c√≥digo gerado: 47%
- Problemas que passaram em todos os testes: 30%
> P.S.: N√£o houveram erros em testes do pr√≥prio dataset

## Conclus√µes: Como isso pode contribuir para nosso projeto?
Isso √© uma √≥tima forma de vermos, na pr√°tica, o impacto que a qualidade do prompt informado impacta na gera√ß√£o de casos de teste! Pode n√£o ser o contexto 100% alocado com o nosso caso (j√° que provavelmente vamos utilizar Selenium em testes black box) mas a ess√™ncia permanece quanto a import√¢ncia do uso de t√©cnicas de Prompt Engineering. Quanto aos resultados, achei que seriam um pouco mais altos, dado o contexto do dataset e a qualidade do c√≥digo gerado, mas realmente em muitos muitos cen√°rios a LLM fez asser√ß√µes muito estranhas... considero como OK, mas poderia ter feito algo melhor, com a qualidade do prompt e do dataset, sabe?

## Contribuindo com o script...
- Se poss√≠vel, tente vers√µes diferentes de prompt para analisar como a LLM se comporta!!
- Minhas maiores indaga√ß√µes em como melhorar esse script consistem em:
  - Como identificar falsos positivos e falsos negativos no script?
  - Como acertar qual √© o culpado do teste falho de forma automatizada?
